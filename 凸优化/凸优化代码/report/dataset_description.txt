# notes
1. 实验应该是遵循[Finding Linear Structure in Large Datasets with Scalable Canonical Correlation Analysis](http://www.jmlr.org/proceedings/papers/v37/maa15.pdf)

# MNIST
## Description in our paper
consists of 60,000 handwritten digits from 0 to 9. Each digit is a image represented by 392 × 392 real values in [0,1]. Here CCA is performed between left half images and right half images. The data matrix is dense but the dimension is fairly small.

## 使用

将loadMNISTImages.m和mnist.m放在同一个目录下，调用mnist，会得到X和Y两个矩阵。


# Penn Tree Bank (PTB)
## Description in our paper
comes from full Wall Street Journal Part of Penn Tree Bank which consists of 1.17 million tokens and a vocabulary size of 43k(Marcus et al., 1993), which has already been used to successfully learn the word embedding by CCA(Dhillon et al., 2011). Here, the task is to learn correlated components between two consecutive words. We only use the top 10,000 most frequent words. Each row of data matrix X is an indicator vector and hence it is very sparse and X⊤X is diagonal.

## Ref
ref: http://www.cis.upenn.edu/~treebank/home.html
Not Free, GG




# URL Reputation
## Description in our paper
URL Reputation dataset contains 2.4 million URLs and 3.2 million features including both host-based features and lexical based features. Each feature is either real valued or binary. For experiments in this section, we follow the setting of (Ma et al., 2015). We use the first 2 million samples, and run CCA between a subset of host based features and a subset of lexical based features to extract the top 20 components.

## Ref
download: https://archive.ics.uci.edu/ml/machine-learning-databases/url/
description: https://archive.ics.uci.edu/ml/datasets/URL+Reputation
http://www.jmlr.org/proceedings/papers/v37/maa15.pdf

## Preprocessing
按论文中说的，取前二百万条数据。由于无法准确获知对应的特征index，将其中
1,3,5,7, ... 共十万个特征作为X
2,4,6,8, ... 共十万个特征作为Y
